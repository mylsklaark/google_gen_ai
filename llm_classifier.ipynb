{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6e13eef3f5d"
   },
   "source": [
    "### Text Classification Using the Google Gemini API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project riffs on the notebook supplied on day four in Google and Kaggle's generative AI course  (see 'notebook' folder). Having trained my own models to complete text classification tasks (see 'coursework_text_classifier' notebook) I wanted to experiment with an LLM to see how the results compared. I've retained and/or reworked some of the text from the original Google/Kaggle notebook so that I can continue to use this as a resource for my own learning and development.\n",
    "\n",
    "In this notebook I use the Gemini API to fine-tune a custom, task-specific model. Fine-tuning can be used for a variety of tasks from classic NLP problems like entity extraction or summarisation, to creative tasks like stylised generation. I fine-tune a model to classify the category of a piece of text (a tweet) into the category it belongs to (a specific natural disaster, or not disaster-realted).\n",
    "\n",
    "This notebook tunes a model with the API. [AI Studio](https://aistudio.google.com/app/tune) also supports creating new tuned models directly in the web UI, allowing you to quickly create and monitor models using data from Google Sheets, Drive or your own files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T20:04:40.108346Z",
     "iopub.status.busy": "2025-04-07T20:04:40.107976Z",
     "iopub.status.idle": "2025-04-07T20:05:02.397261Z",
     "shell.execute_reply": "2025-04-07T20:05:02.395226Z",
     "shell.execute_reply.started": "2025-04-07T20:04:40.108310Z"
    },
    "id": "9wafTyEH1_xF",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip uninstall -qqy jupyterlab  # Remove unused conflicting packages\n",
    "!pip install -U -q \"google-genai==1.7.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.0'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from collections.abc import Iterable\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "import ndjson\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.rich import tqdm as tqdmr\n",
    "\n",
    "# Application-specific / service-specific imports\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from google.api_core import retry\n",
    "\n",
    "genai.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4bYX2T72ScK"
   },
   "source": [
    "### Set up the API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GEMINI_API_KEY not found in environment variables.\")\n",
    "\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CqVA5QFO6n4z"
   },
   "source": [
    "### Explore available models\n",
    "\n",
    "I'll be using the [`TunedModel.create`](https://ai.google.dev/api/tuning#method:-tunedmodels.create) API method to start the fine-tuning job and create my custom model. You can find a model that supports it through the [`models.list`](https://ai.google.dev/api/models#method:-models.list) endpoint. You can also find more information about tuning models in [the model tuning docs](https://ai.google.dev/gemini-api/docs/model-tuning/tutorial?lang=python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T20:06:09.785087Z",
     "iopub.status.busy": "2025-04-07T20:06:09.783809Z",
     "iopub.status.idle": "2025-04-07T20:06:09.841148Z",
     "shell.execute_reply": "2025-04-07T20:06:09.839887Z",
     "shell.execute_reply.started": "2025-04-07T20:06:09.785024Z"
    },
    "id": "coEacWAB6o0G",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/gemini-1.5-flash-001-tuning\n"
     ]
    }
   ],
   "source": [
    "for model in client.models.list():\n",
    "    if \"createTunedModel\" in model.supported_actions:\n",
    "        print(model.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "peFm0w_0c1CO"
   },
   "source": [
    "## Download the dataset\n",
    "\n",
    "For this activity, I once more use the 'Disaster Tweet Corpus 2020' data set. The entire data set consists of 48 newline delimited JSON (ndjson) files covering 10 different disaster types. These include earthquakes, tsunamis, and humanmade industrial disasters. Each data set contains an even split of disaster-related and non-disaster-related tweets. They are human-labelled, with a '1' indicating that the tweet relates to a disaster, and a '0' indicating that the opposite is true. As well as the tweet text and classification, a third variable, the user id, is also included. The data sets are intended to be used for benchmarking for filtering algorithms. Therefore, they meet the needs of the scope of this project. I used a curated selection of these files, choosing one to represent each of the five disasters: earthquake, flood, hurricane, tornado, and wildfire. I have not repeated the formal references in this project, so please look to the original notebook for said information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the files but ignore the ID column as it's unecessary for this work.\n",
    "\n",
    "data_dir = 'data'\n",
    "all_data = []\n",
    "\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith('.ndjson'):\n",
    "        \n",
    "        label_word = filename.split('-')[0].lower()\n",
    "        \n",
    "        filepath = os.path.join(data_dir, filename)\n",
    "        \n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            tweets = ndjson.load(f)\n",
    "            \n",
    "            for tweet in tweets:\n",
    "                text = tweet.get('text', '')\n",
    "                relevance = tweet.get('relevance', 0)\n",
    "                \n",
    "                if relevance == 1:\n",
    "                    label = label_word\n",
    "                else:\n",
    "                    label =  'non-disaster'\n",
    "                    \n",
    "                all_data.append({'text': text, 'label': label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'non-disaster': 10048, 'hurricane': 3837, 'tornado': 1876, 'flood': 1797, 'earthquake': 1673, 'wildfire': 865})\n"
     ]
    }
   ],
   "source": [
    "# Check the labels and their respective counts.\n",
    "\n",
    "label_counts = Counter(d['label'] for d in all_data)\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ipafe6ptZFjt"
   },
   "source": [
    "Here's what a single row looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"cruuuuud. tornado warning for denton county. and the tv isn't working. and the dallas news sites aren't helpful. cruuuuud.\", 'label': 'tornado'}\n"
     ]
    }
   ],
   "source": [
    "print(all_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03lDs1O4ZQ0-"
   },
   "source": [
    "## Prepare the dataset\n",
    "\n",
    "In the Google/Kaggle example, they state:\n",
    "\n",
    "    \"This pre-processing removes personal information, which can be used to \"shortcut\" to known users of a forum, and formats the text to appear a bit more like regular text and less like a newsgroup post (e.g. by removing the mail headers). This normalisation allows the model to generalise to regular text and not over-depend on specific fields. If your input data is always going to be newsgroup posts, it may be helpful to leave this structure in place if they provide genuine signals.\"\n",
    "\n",
    "The approach to preprocessing the text I've adopted below is much more light touch than when I trained my own models. For example, I've opted not to tokenize and lemmatize the text as this will likely deny the LLM useful information when classifying the text. I think leaving in some of the symbols is possibly not advisable, but given that this is an experiment I want to see how it performs with those included. In any case, I can come back and edit the preprocessing in an attempt to improve the model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go back and adjust some of this if results are not satisfactory.\n",
    "\n",
    "def clean_text_for_llm(text):\n",
    "    \"\"\"\n",
    "    Preprocesses a text string for input into a large language model (LLM).\n",
    "\n",
    "    This function performs the following cleaning steps:\n",
    "    - Converts all characters to lowercase\n",
    "    - Replaces URLs with a <URL> placeholder\n",
    "    - Removes specific special characters and encoding noise\n",
    "    - Collapses excess whitespace into single spaces\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to clean.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned and normalized text.\n",
    "    \"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace URLs with placeholder\n",
    "    text = re.sub(r'https?://\\S+','<URL>', text)\n",
    "    \n",
    "    # Fix special characters or encoding noise\n",
    "    text = re.sub(r'[â€—\\x01]', '', text)\n",
    "    \n",
    "    # Remove excess whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = []\n",
    "\n",
    "for tweet in all_data:\n",
    "    cleaned_tweet = {\n",
    "        'text': clean_text_for_llm(tweet['text']),\n",
    "        'label': tweet['label']\n",
    "    }\n",
    "    cleaned_data.append(cleaned_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = pd.DataFrame(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cruuuuud. tornado warning for denton county. a...</td>\n",
       "      <td>tornado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tornado warning for tarrant county. storm spot...</td>\n",
       "      <td>tornado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tornado on the ground in azle headed right tow...</td>\n",
       "      <td>tornado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>us tornado toll rises to &lt;number&gt; (afp) &lt;url&gt;</td>\n",
       "      <td>tornado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tornado sirens. this is scary.</td>\n",
       "      <td>tornado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20091</th>\n",
       "      <td>&lt;number&gt; goes out to &lt;user&gt; @paytonlund &lt;user&gt;...</td>\n",
       "      <td>non-disaster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20092</th>\n",
       "      <td>the perception of doctors has undergone a dram...</td>\n",
       "      <td>non-disaster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20093</th>\n",
       "      <td>birmingham, live on tour. &lt;url&gt;</td>\n",
       "      <td>non-disaster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20094</th>\n",
       "      <td>february &lt;number&gt; h is international polar bea...</td>\n",
       "      <td>non-disaster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20095</th>\n",
       "      <td>what ima do what ima do , ima beat my dick ima...</td>\n",
       "      <td>non-disaster</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20096 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text         label\n",
       "0      cruuuuud. tornado warning for denton county. a...       tornado\n",
       "1      tornado warning for tarrant county. storm spot...       tornado\n",
       "2      tornado on the ground in azle headed right tow...       tornado\n",
       "3          us tornado toll rises to <number> (afp) <url>       tornado\n",
       "4                         tornado sirens. this is scary.       tornado\n",
       "...                                                  ...           ...\n",
       "20091  <number> goes out to <user> @paytonlund <user>...  non-disaster\n",
       "20092  the perception of doctors has undergone a dram...  non-disaster\n",
       "20093                    birmingham, live on tour. <url>  non-disaster\n",
       "20094  february <number> h is international polar bea...  non-disaster\n",
       "20095  what ima do what ima do , ima beat my dick ima...  non-disaster\n",
       "\n",
       "[20096 rows x 2 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When sampling the data, I will keep 50 rows for each category for training. Google says:\n",
    "\n",
    "    Note that this is even fewer than the Keras example, as this technique (parameter-efficient fine-tuning, or PEFT) updates a relatively small number of parameters and does not require training a new model or updating the large model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(df, num_samples, labels_to_keep):\n",
    "    \"\"\"\n",
    "    Samples a specified number of examples per label from a DataFrame.\n",
    "\n",
    "    Filters the DataFrame to include only the specified labels, then \n",
    "    randomly samples a fixed number of examples for each label category.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame, expected to contain a 'label' column.\n",
    "        num_samples (int): The number of samples to draw for each label.\n",
    "        labels_to_keep (Iterable): A list-like object of labels to include in the sample.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame containing the sampled rows, \n",
    "                      with the 'label' column cast to categorical type.\n",
    "    \"\"\"\n",
    "    df = df[df[\"label\"].isin(labels_to_keep)]\n",
    "    \n",
    "    df = (\n",
    "        df.groupby(\"label\", observed=False)[df.columns]\n",
    "        .apply(lambda x: x.sample(num_samples, random_state=42))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    df[\"label\"] = df[\"label\"].astype(\"category\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_to_keep = ['non-disaster', 'hurricane', 'tornado', 'flood', 'earthquake','wildfire']\n",
    "train_num_samples = 50 \n",
    "test_num_samples = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_raw, df_test_raw = train_test_split(\n",
    "    df_cleaned,\n",
    "    test_size=0.2,\n",
    "    stratify=df_cleaned['label'],\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = sample_data(df_train_raw, train_num_samples, labels_to_keep) \n",
    "df_test = sample_data(df_test_raw, test_num_samples, labels_to_keep) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for all in sydney and most of illawarra <user> sydney water statement: <hashtag> bluemountains <hashtag> bushfires <url> <hashtag> nswfires”'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.iloc[59]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate baseline performance\n",
    "\n",
    "First, I  perform an evaluation on the available models to ensure I can measure how much the tuning helps.\n",
    "\n",
    "Below is a single sample row to use for visual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "napa valley wineries sustain damage from <number> earthquake la times: <url> <url> via @carolcnn\n",
      "---\n",
      "Label: earthquake\n"
     ]
    }
   ],
   "source": [
    "sample_idx = 0\n",
    "sample_row = df_train.iloc[sample_idx]['text']\n",
    "sample_label = df_train.iloc[sample_idx]['label']\n",
    "\n",
    "print(sample_row)\n",
    "print('---')\n",
    "print('Label:', sample_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing the text directly in as a prompt does not yield the desired results. The model will attempt to respond to the message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This looks like a social media post about Napa Valley wineries being damaged by an earthquake. To give you the most helpful information, I need more details.  \n",
      "\n",
      "Please tell me:\n",
      "\n",
      "* **What number is missing?**  Is it the magnitude of the earthquake? The number of wineries damaged? \n",
      "* **What are the URLs?** I need the full URLs to access the articles.\n",
      "\n",
      "Once I have this information, I can help you find out more about the earthquake and its impact on Napa Valley wineries. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-1.5-flash-001\", contents=sample_row)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt engineering techniques induce the model to perform the desired task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tweet originates from an **earthquake**. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ask the model directly in a zero-shot prompt.\n",
    "\n",
    "prompt = \"From what natural disaster does the following tweet originate?\"\n",
    "baseline_response = client.models.generate_content(\n",
    "    model=\"gemini-1.5-flash-001\",\n",
    "    contents=[prompt, sample_row])\n",
    "print(baseline_response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This technique still produces quite a verbose response. I try and parse out the relevant text, and then refine the prompt even further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earthquake\n",
      "\n",
      "Incorrect.\n"
     ]
    }
   ],
   "source": [
    "from google.api_core import retry\n",
    "\n",
    "# You can use a system instruction to do more direct prompting, and get a\n",
    "# more succinct answer.\n",
    "\n",
    "system_instruct = \"\"\"\n",
    "You are a classification service. You will be passed input that represents\n",
    "a tweet from a natural disaster and you must respond with the natural disaster from which the tweet\n",
    "originates.\n",
    "\"\"\"\n",
    "\n",
    "# Define a helper to retry when per-minute quota is reached.\n",
    "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
    "\n",
    "# If you want to evaluate your own technique, replace this body of this function\n",
    "# with your model, prompt and other code and return the predicted answer.\n",
    "@retry.Retry(predicate=is_retriable)\n",
    "def predict_label(post: str) -> str:\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-1.5-flash-001\",\n",
    "        config=types.GenerateContentConfig(\n",
    "            system_instruction=system_instruct),\n",
    "        contents=post)\n",
    "\n",
    "    rc = response.candidates[0]\n",
    "\n",
    "    # Any errors, filters, recitation, etc we can mark as a general error\n",
    "    if rc.finish_reason.name != \"STOP\":\n",
    "        return \"(error)\"\n",
    "    else:\n",
    "        # Clean up the response.\n",
    "        return response.text.strip()\n",
    "\n",
    "\n",
    "prediction = predict_label(sample_row)\n",
    "\n",
    "print(prediction)\n",
    "print()\n",
    "print(\"Correct!\" if prediction == sample_label else \"Incorrect.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By dint of beginning in uppercase, it wrongly states that this classification is incorrect. I refine the prompt to resolve this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "earthquake\n",
      "\n",
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "from google.api_core import retry\n",
    "\n",
    "# You can use a system instruction to do more direct prompting, and get a\n",
    "# more succinct answer.\n",
    "\n",
    "system_instruct = \"\"\"\n",
    "You are a classification service. You will be passed input that represents\n",
    "a tweet from a natural disaster and you must respond with the natural disaster from which the tweet\n",
    "originates. Your response must be lowercase.\n",
    "\"\"\"\n",
    "\n",
    "# Define a helper to retry when per-minute quota is reached.\n",
    "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
    "\n",
    "# If you want to evaluate your own technique, replace this body of this function\n",
    "# with your model, prompt and other code and return the predicted answer.\n",
    "@retry.Retry(predicate=is_retriable)\n",
    "def predict_label(post: str) -> str:\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-1.5-flash-001\",\n",
    "        config=types.GenerateContentConfig(\n",
    "            system_instruction=system_instruct),\n",
    "        contents=post)\n",
    "\n",
    "    rc = response.candidates[0]\n",
    "\n",
    "    # Any errors, filters, recitation, etc we can mark as a general error\n",
    "    if rc.finish_reason.name != \"STOP\":\n",
    "        return \"(error)\"\n",
    "    else:\n",
    "        # Clean up the response.\n",
    "        return response.text.strip()\n",
    "\n",
    "\n",
    "prediction = predict_label(sample_row)\n",
    "\n",
    "print(prediction)\n",
    "print()\n",
    "print(\"Correct!\" if prediction == sample_label else \"Incorrect.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I run a short evaluation using the function defined above. The test set is further sampled to ensure the experiment runs smoothly on the API's free tier. In practice you would evaluate over the whole set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_baseline_eval = sample_data(df_test, 2, labels_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T20:09:39.240210Z",
     "iopub.status.busy": "2025-04-07T20:09:39.239604Z",
     "iopub.status.idle": "2025-04-07T20:09:45.411327Z",
     "shell.execute_reply": "2025-04-07T20:09:45.409810Z",
     "shell.execute_reply.started": "2025-04-07T20:09:39.240149Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "052547398036440db0ce9192881a5092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 50.00%\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "from tqdm.rich import tqdm as tqdmr\n",
    "import warnings\n",
    "\n",
    "# Enable tqdm features on Pandas.\n",
    "tqdmr.pandas()\n",
    "\n",
    "# But suppress the experimental warning\n",
    "warnings.filterwarnings(\"ignore\", category=tqdm.TqdmExperimentalWarning)\n",
    "\n",
    "# Further sample the test data to be mindful of the free-tier quota.\n",
    "df_baseline_eval = sample_data(df_test, 2, labels_to_keep)\n",
    "\n",
    "# Make predictions using the sampled data.\n",
    "df_baseline_eval['prediction'] = df_baseline_eval['text'].progress_apply(predict_label)\n",
    "\n",
    "# And calculate the accuracy.\n",
    "accuracy = (df_baseline_eval[\"label\"] == df_baseline_eval[\"prediction\"]).sum() / len(df_baseline_eval)\n",
    "print(f\"Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the dataframe to compare the predictions with the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T20:09:49.542040Z",
     "iopub.status.busy": "2025-04-07T20:09:49.541564Z",
     "iopub.status.idle": "2025-04-07T20:09:49.555908Z",
     "shell.execute_reply": "2025-04-07T20:09:49.554367Z",
     "shell.execute_reply.started": "2025-04-07T20:09:49.542000Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>california usa downey » &lt;url&gt; &lt;hashtag&gt; sfgate...</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>earthquake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>totally felt the last &lt;hashtag&gt; napaquake afte...</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>earthquake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;user&gt; discuss how to help flood,police author...</td>\n",
       "      <td>flood</td>\n",
       "      <td>(error)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>monsoon floods in nepal and india cause &lt;numbe...</td>\n",
       "      <td>flood</td>\n",
       "      <td>flood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>usgs:m &lt;number&gt; - &lt;number&gt; m wnw of rincon, pu...</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>earthquake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;user&gt; cast members raise &lt;number&gt; 000 for pue...</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>hurricane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;user&gt; you deserve win guys! we love you! you ...</td>\n",
       "      <td>non-disaster</td>\n",
       "      <td>This is not a tweet about a natural disaster.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\"i've been happy for two days. now it's time t...</td>\n",
       "      <td>non-disaster</td>\n",
       "      <td>This tweet does not describe a natural disaster.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>i live in joplin mo where the &lt;number&gt; tornado...</td>\n",
       "      <td>tornado</td>\n",
       "      <td>tornado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>my dad's been in &lt;user&gt; &lt;user&gt; today because h...</td>\n",
       "      <td>tornado</td>\n",
       "      <td>tornado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>. &lt;user&gt; links nsw bush fires with likely &lt;has...</td>\n",
       "      <td>wildfire</td>\n",
       "      <td>bushfire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>nsw on fire, vic saturated. what is the &lt;hasht...</td>\n",
       "      <td>wildfire</td>\n",
       "      <td>bushfire</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text         label  \\\n",
       "0   california usa downey » <url> <hashtag> sfgate...    earthquake   \n",
       "1   totally felt the last <hashtag> napaquake afte...    earthquake   \n",
       "2   <user> discuss how to help flood,police author...         flood   \n",
       "3   monsoon floods in nepal and india cause <numbe...         flood   \n",
       "4   usgs:m <number> - <number> m wnw of rincon, pu...     hurricane   \n",
       "5   <user> cast members raise <number> 000 for pue...     hurricane   \n",
       "6   <user> you deserve win guys! we love you! you ...  non-disaster   \n",
       "7   \"i've been happy for two days. now it's time t...  non-disaster   \n",
       "8   i live in joplin mo where the <number> tornado...       tornado   \n",
       "9   my dad's been in <user> <user> today because h...       tornado   \n",
       "10  . <user> links nsw bush fires with likely <has...      wildfire   \n",
       "11  nsw on fire, vic saturated. what is the <hasht...      wildfire   \n",
       "\n",
       "                                          prediction  \n",
       "0                                         earthquake  \n",
       "1                                         earthquake  \n",
       "2                                            (error)  \n",
       "3                                              flood  \n",
       "4                                         earthquake  \n",
       "5                                          hurricane  \n",
       "6      This is not a tweet about a natural disaster.  \n",
       "7   This tweet does not describe a natural disaster.  \n",
       "8                                            tornado  \n",
       "9                                            tornado  \n",
       "10                                          bushfire  \n",
       "11                                          bushfire  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_baseline_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ok7ugrLzcghX"
   },
   "source": [
    "## Tune a custom model\n",
    "\n",
    "In this example I use tuning to create a model that requires no prompting or system instructions and outputs succinct text from the classes provided in the training data.\n",
    "\n",
    "The data contains both input text (the processed tweets) and output text (the category, or disaster), that I can use to start tuning a model.\n",
    "\n",
    "When calling `tune()`, you can specify model tuning hyperparameters too:\n",
    " - `epoch_count`: defines how many times to loop through the data,\n",
    " - `batch_size`: defines how many rows to process in a single step, and\n",
    " - `learning_rate`: defines the scaling factor for updating model weights at each step.\n",
    "\n",
    "You can also choose to omit them and use the defaults. [Learn more](https://developers.google.com/machine-learning/crash-course/linear-regression/hyperparameters) about these parameters and how they work.\n",
    "\n",
    "### Go back and look at this to see how I can adjust for the purposes of my experiment.\n",
    "\n",
    "For Google's example, they chose parameters by running some tuning jobs and selecting parameters that converged efficiently.\n",
    "\n",
    "This example will start a new tuning job, but only if one does not already exist. This allows you to leave this codelab and come back later - re-running this step will find your last model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T20:11:09.492965Z",
     "iopub.status.busy": "2025-04-07T20:11:09.492502Z",
     "iopub.status.idle": "2025-04-07T20:11:12.297005Z",
     "shell.execute_reply": "2025-04-07T20:11:12.295675Z",
     "shell.execute_reply.started": "2025-04-07T20:11:09.492924Z"
    },
    "id": "pWOZlspfY8dV",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Convert the data frame into a dataset suitable for tuning.\n",
    "#input_data = {'examples': \n",
    "#    df_train[['text', 'label']]\n",
    "#      .rename(columns={'text': 'textInput', 'label': 'output'})\n",
    "#      .to_dict(orient='records')\n",
    "# }\n",
    "\n",
    "# If you are re-running this lab, add your model_id here.\n",
    "#model_id = None\n",
    "\n",
    "# Or try and find a recent tuning job.\n",
    "#if not model_id:\n",
    "#  queued_model = None\n",
    "  # Newest models first.\n",
    "#  for m in reversed(client.tunings.list()):\n",
    "    # Only look at newsgroup classification models.\n",
    "#    if m.name.startswith('tunedModels/newsgroup-classification-model'):\n",
    "      # If there is a completed model, use the first (newest) one.\n",
    "#      if m.state.name == 'JOB_STATE_SUCCEEDED':\n",
    "#        model_id = m.name\n",
    "#        print('Found existing tuned model to reuse.')\n",
    "#        break\n",
    "\n",
    "#      elif m.state.name == 'JOB_STATE_RUNNING' and not queued_model:\n",
    "        # If there's a model still queued, remember the most recent one.\n",
    "#        queued_model = m.name\n",
    "#  else:\n",
    "#    if queued_model:\n",
    "#      model_id = queued_model\n",
    "#      print('Found queued model, still waiting.')\n",
    "\n",
    "\n",
    "# Upload the training data and queue the tuning job.\n",
    "#if not model_id:\n",
    "#    tuning_op = client.tunings.tune(\n",
    "#        base_model=\"models/gemini-1.5-flash-001-tuning\",\n",
    "#        training_dataset=input_data,\n",
    "#        config=types.CreateTuningJobConfig(\n",
    "#            tuned_model_display_name=\"Newsgroup classification model\",\n",
    "#            batch_size=16,\n",
    "#            epoch_count=2,\n",
    "#        ),\n",
    "#    )\n",
    "#\n",
    "#    print(tuning_op.state)\n",
    "#    model_id = tuning_op.name\n",
    "#\n",
    "#print(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JobState.JOB_STATE_QUEUED\n",
      "New tuning job started: tunedModels/tweet-classification-8hm6pyjgdtwr\n"
     ]
    }
   ],
   "source": [
    "# Prepare your new dataset\n",
    "input_data = {\n",
    "    'examples': df_train[['text', 'label']]\n",
    "        .rename(columns={'text': 'textInput', 'label': 'output'})\n",
    "        .to_dict(orient='records')\n",
    "}\n",
    "\n",
    "# Set model_id to None to ensure a new job is created\n",
    "model_id = None\n",
    "\n",
    "# Always create a new tuning job\n",
    "tuning_op = client.tunings.tune(\n",
    "    base_model=\"models/gemini-1.5-flash-001-tuning\",\n",
    "    training_dataset=input_data,\n",
    "    config=types.CreateTuningJobConfig(\n",
    "        tuned_model_display_name=\"Tweet classification\",  # You can rename this if you'd like\n",
    "        batch_size=16,\n",
    "        epoch_count=2,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(tuning_op.state)\n",
    "model_id = tuning_op.name\n",
    "\n",
    "print(f\"New tuning job started: {model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQ3YZ2MBubCY"
   },
   "source": [
    "This has created a tuning job that will run in the background. To inspect the progress of the tuning job, run this cell to plot the current status and loss curve. Once the status reaches `ACTIVE`, tuning is complete and the model is ready to use.\n",
    "\n",
    "Tuning jobs are queued, so it may look like no training steps have been taken initially but it will progress. Tuning can take anywhere from a few minutes to multiple hours, depending on factors like your dataset size and how busy the tuning infrastrature is.\n",
    "\n",
    "It is safe to stop this cell at any point. It will not stop the tuning job.\n",
    "\n",
    "Have a look at the [Search grounding](https://www.kaggle.com/code/markishere/day-4-google-search-grounding/) codelab. If you want to try tuning a local LLM, check out [the fine-tuning guides for tuning a Gemma model](https://ai.google.dev/gemma/docs/tune)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T20:11:53.951742Z",
     "iopub.status.busy": "2025-04-07T20:11:53.951251Z",
     "iopub.status.idle": "2025-04-07T20:16:55.949376Z",
     "shell.execute_reply": "2025-04-07T20:16:55.948067Z",
     "shell.execute_reply.started": "2025-04-07T20:11:53.951707Z"
    },
    "id": "c4ef5f13692d",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JobState.JOB_STATE_RUNNING\n",
      "JobState.JOB_STATE_RUNNING\n",
      "JobState.JOB_STATE_RUNNING\n",
      "JobState.JOB_STATE_RUNNING\n",
      "Done! The model state is: JOB_STATE_SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "MAX_WAIT = datetime.timedelta(minutes=10)\n",
    "\n",
    "while not (tuned_model := client.tunings.get(name=model_id)).has_ended:\n",
    "\n",
    "    print(tuned_model.state)\n",
    "    time.sleep(60)\n",
    "\n",
    "    # Don't wait too long. Use a public model if this is going to take a while.\n",
    "    if datetime.datetime.now(datetime.timezone.utc) - tuned_model.create_time > MAX_WAIT:\n",
    "        print(\"Taking a shortcut, using a previously prepared model.\")\n",
    "        model_id = \"tunedModels/newsgroup-classification-model-ltenbi1b\"\n",
    "        tuned_model = client.tunings.get(name=model_id)\n",
    "        break\n",
    "\n",
    "\n",
    "print(f\"Done! The model state is: {tuned_model.state.name}\")\n",
    "\n",
    "if not tuned_model.has_succeeded and tuned_model.error:\n",
    "    print(\"Error:\", tuned_model.error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-qiIdK4u80z"
   },
   "source": [
    "## Use the new model\n",
    "\n",
    "Now that I have a  tuned model, I can try it out with custom data. I use the same API as a normal Gemini API interaction, but specify the new model as the model name, which starts with `tunedModels/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T20:18:18.791994Z",
     "iopub.status.busy": "2025-04-07T20:18:18.791518Z",
     "iopub.status.idle": "2025-04-07T20:18:20.592845Z",
     "shell.execute_reply": "2025-04-07T20:18:20.591502Z",
     "shell.execute_reply.started": "2025-04-07T20:18:18.791953Z"
    },
    "id": "hyO2-MXLvM6a",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flood\n"
     ]
    }
   ],
   "source": [
    "model_id = 'tunedModels/tweet-classification-6oock2yj2wg0'\n",
    "\n",
    "new_text = \"\"\"\n",
    "The water came in the door.\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=model_id, contents=new_text)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xajLek9DySH_"
   },
   "source": [
    "### Evaluation\n",
    "\n",
    "You can see that the model outputs labels that correspond to those in the training data, and without any system instructions or prompting, which is already a significant improvement. Now see how well it performs on the test set.\n",
    "\n",
    "Note that there is no parallelism in this example; classifying the test sub-set will take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T20:18:28.719288Z",
     "iopub.status.busy": "2025-04-07T20:18:28.718009Z",
     "iopub.status.idle": "2025-04-07T20:21:01.855757Z",
     "shell.execute_reply": "2025-04-07T20:21:01.854445Z",
     "shell.execute_reply.started": "2025-04-07T20:18:28.719223Z"
    },
    "id": "6T2Y3ZApvbMw",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36b1baa9fdff4d6dbf08949a7c3e415b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 79.17%\n"
     ]
    }
   ],
   "source": [
    "@retry.Retry(predicate=is_retriable)\n",
    "def classify_text(text: str) -> str:\n",
    "    \"\"\"Classify the provided text into a known category.\"\"\"\n",
    "    response = client.models.generate_content(\n",
    "        model=model_id, contents=text)\n",
    "    rc = response.candidates[0]\n",
    "\n",
    "    # Any errors, filters, recitation, etc we can mark as a general error\n",
    "    if rc.finish_reason.name != \"STOP\":\n",
    "        return \"(error)\"\n",
    "    else:\n",
    "        return rc.content.parts[0].text\n",
    "\n",
    "\n",
    "# The sampling here is just to minimise your quota usage. If you can, you should\n",
    "# evaluate the whole test set with `df_model_eval = df_test.copy()`.\n",
    "df_model_eval = sample_data(df_test, 4, labels_to_keep)\n",
    "\n",
    "df_model_eval[\"prediction\"] = df_model_eval[\"text\"].progress_apply(classify_text)\n",
    "\n",
    "accuracy = (df_model_eval[\"label\"] == df_model_eval[\"prediction\"]).sum() / len(df_model_eval)\n",
    "print(f\"Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>california usa downey » &lt;url&gt; &lt;hashtag&gt; sfgate...</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>earthquake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>totally felt the last &lt;hashtag&gt; napaquake afte...</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>earthquake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>if you really want to help &lt;hashtag&gt; drinknapa...</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>earthquake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;user&gt; there was also an earthquake in califor...</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>earthquake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;user&gt; discuss how to help flood,police author...</td>\n",
       "      <td>flood</td>\n",
       "      <td>(error)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>monsoon floods in nepal and india cause &lt;numbe...</td>\n",
       "      <td>flood</td>\n",
       "      <td>flood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>many dead in nepal and india floods &lt;url&gt; @rar...</td>\n",
       "      <td>flood</td>\n",
       "      <td>flood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&lt;hashtag&gt; news &lt;hashtag&gt; mostrecent hundreds d...</td>\n",
       "      <td>flood</td>\n",
       "      <td>flood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>usgs:m &lt;number&gt; - &lt;number&gt; m wnw of rincon, pu...</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>earthquake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;user&gt; cast members raise &lt;number&gt; 000 for pue...</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>hurricane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>residents feel like &lt;number&gt; d class citizens ...</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>hurricane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>weather blog: hurricane maria update &lt;url&gt; &lt;url&gt;</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>hurricane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>&lt;user&gt; you deserve win guys! we love you! you ...</td>\n",
       "      <td>non-disaster</td>\n",
       "      <td>(error)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>\"i've been happy for two days. now it's time t...</td>\n",
       "      <td>non-disaster</td>\n",
       "      <td>(error)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>who ever tried to count me out cant count &lt;rep...</td>\n",
       "      <td>non-disaster</td>\n",
       "      <td>non-disaster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>the latest boasting in my weakness daily! &lt;url...</td>\n",
       "      <td>non-disaster</td>\n",
       "      <td>non-disaster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>i live in joplin mo where the &lt;number&gt; tornado...</td>\n",
       "      <td>tornado</td>\n",
       "      <td>tornado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>my dad's been in &lt;user&gt; &lt;user&gt; today because h...</td>\n",
       "      <td>tornado</td>\n",
       "      <td>tornado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ordered a pizza. &lt;hashtag&gt; selfish &lt;hashtag&gt; t...</td>\n",
       "      <td>tornado</td>\n",
       "      <td>tornado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>&lt;user&gt; oh you guys are used to tornado sirens....</td>\n",
       "      <td>tornado</td>\n",
       "      <td>tornado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>. &lt;user&gt; links nsw bush fires with likely &lt;has...</td>\n",
       "      <td>wildfire</td>\n",
       "      <td>(error)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>nsw on fire, vic saturated. what is the &lt;hasht...</td>\n",
       "      <td>wildfire</td>\n",
       "      <td>wildfire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>&lt;hashtag&gt; nswrfs &amp;amp; &lt;hashtag&gt; frnsw crews o...</td>\n",
       "      <td>wildfire</td>\n",
       "      <td>wildfire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>&lt;hashtag&gt; nswfires no &lt;number&gt; &lt;number&gt; &lt;numbe...</td>\n",
       "      <td>wildfire</td>\n",
       "      <td>wildfire</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text         label  \\\n",
       "0   california usa downey » <url> <hashtag> sfgate...    earthquake   \n",
       "1   totally felt the last <hashtag> napaquake afte...    earthquake   \n",
       "2   if you really want to help <hashtag> drinknapa...    earthquake   \n",
       "3   <user> there was also an earthquake in califor...    earthquake   \n",
       "4   <user> discuss how to help flood,police author...         flood   \n",
       "5   monsoon floods in nepal and india cause <numbe...         flood   \n",
       "6   many dead in nepal and india floods <url> @rar...         flood   \n",
       "7   <hashtag> news <hashtag> mostrecent hundreds d...         flood   \n",
       "8   usgs:m <number> - <number> m wnw of rincon, pu...     hurricane   \n",
       "9   <user> cast members raise <number> 000 for pue...     hurricane   \n",
       "10  residents feel like <number> d class citizens ...     hurricane   \n",
       "11   weather blog: hurricane maria update <url> <url>     hurricane   \n",
       "12  <user> you deserve win guys! we love you! you ...  non-disaster   \n",
       "13  \"i've been happy for two days. now it's time t...  non-disaster   \n",
       "14  who ever tried to count me out cant count <rep...  non-disaster   \n",
       "15  the latest boasting in my weakness daily! <url...  non-disaster   \n",
       "16  i live in joplin mo where the <number> tornado...       tornado   \n",
       "17  my dad's been in <user> <user> today because h...       tornado   \n",
       "18  ordered a pizza. <hashtag> selfish <hashtag> t...       tornado   \n",
       "19  <user> oh you guys are used to tornado sirens....       tornado   \n",
       "20  . <user> links nsw bush fires with likely <has...      wildfire   \n",
       "21  nsw on fire, vic saturated. what is the <hasht...      wildfire   \n",
       "22  <hashtag> nswrfs &amp; <hashtag> frnsw crews o...      wildfire   \n",
       "23  <hashtag> nswfires no <number> <number> <numbe...      wildfire   \n",
       "\n",
       "      prediction  \n",
       "0     earthquake  \n",
       "1     earthquake  \n",
       "2     earthquake  \n",
       "3     earthquake  \n",
       "4        (error)  \n",
       "5          flood  \n",
       "6          flood  \n",
       "7          flood  \n",
       "8     earthquake  \n",
       "9      hurricane  \n",
       "10     hurricane  \n",
       "11     hurricane  \n",
       "12       (error)  \n",
       "13       (error)  \n",
       "14  non-disaster  \n",
       "15  non-disaster  \n",
       "16       tornado  \n",
       "17       tornado  \n",
       "18       tornado  \n",
       "19       tornado  \n",
       "20       (error)  \n",
       "21      wildfire  \n",
       "22      wildfire  \n",
       "23      wildfire  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare token usage\n",
    "\n",
    "AI Studio and the Gemini API provide model tuning at no cost, however normal limits and charges apply for *use* of a tuned model.\n",
    "\n",
    "The size of the input prompt and other generation config like system instructions, as well as the number of generated output tokens, all contribute to the overall cost of a request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T20:21:29.203357Z",
     "iopub.status.busy": "2025-04-07T20:21:29.202934Z",
     "iopub.status.idle": "2025-04-07T20:21:29.529225Z",
     "shell.execute_reply": "2025-04-07T20:21:29.527922Z",
     "shell.execute_reply.started": "2025-04-07T20:21:29.203319Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System instructed baseline model: 69 (input)\n",
      "Tuned model: 25 (input)\n",
      "Token savings: 176.00%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the *input* cost of the baseline model with system instructions.\n",
    "sysint_tokens = client.models.count_tokens(\n",
    "    model='gemini-1.5-flash-001', contents=[system_instruct, sample_row]\n",
    ").total_tokens\n",
    "print(f'System instructed baseline model: {sysint_tokens} (input)')\n",
    "\n",
    "# Calculate the input cost of the tuned model.\n",
    "tuned_tokens = client.models.count_tokens(model=tuned_model.base_model, contents=sample_row).total_tokens\n",
    "print(f'Tuned model: {tuned_tokens} (input)')\n",
    "\n",
    "savings = (sysint_tokens - tuned_tokens) / tuned_tokens\n",
    "print(f'Token savings: {savings:.2%}')  # Note that this is only n=1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The earlier verbose model also produced more output tokens than needed for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T20:21:44.909725Z",
     "iopub.status.busy": "2025-04-07T20:21:44.909294Z",
     "iopub.status.idle": "2025-04-07T20:21:45.762020Z",
     "shell.execute_reply": "2025-04-07T20:21:45.760519Z",
     "shell.execute_reply.started": "2025-04-07T20:21:44.909691Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (verbose) output tokens: 9\n",
      "Tuned output tokens: 2\n"
     ]
    }
   ],
   "source": [
    "baseline_token_output = baseline_response.usage_metadata.candidates_token_count\n",
    "print('Baseline (verbose) output tokens:', baseline_token_output)\n",
    "\n",
    "tuned_model_output = client.models.generate_content(\n",
    "    model=model_id, contents=sample_row)\n",
    "tuned_tokens_output = tuned_model_output.usage_metadata.candidates_token_count\n",
    "print('Tuned output tokens:', tuned_tokens_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6c1204a5d0ab"
   },
   "source": [
    "## Next steps\n",
    "\n",
    "While the accuracy here is good given the relatively small amount of time I have put into tuning the model, this is not as accurate as the models that I have trained myself. I'm confident that I could increase the accuracy of this classifier through further feature engineering, hyperparameter tuning, or reworking the system instruction. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "day-4-fine-tuning-a-custom-model.ipynb",
   "toc_visible": true
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
